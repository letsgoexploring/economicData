{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Beveridge Curve Data \n",
    "\n",
    "We construct monthly unemploment rate and vacancy rate series for the US from April 1929 through the most recently available date. Our methodology is based on the approach described in Petrosky-Nadeau and Zhang (2013): https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2241695\n",
    "\n",
    "1. This Notebook is compatible with Python 2 and 3.\n",
    "\n",
    "2. **This notebook requires the X-13ARIMA-SEATS binary**. Binaries for Windows and Linux/Unix are available from https://www.census.gov/srd/www/x13as/. To compile X-13 for Mac OS X, see the instructions here: https://github.com/christophsax/seasonal/wiki/Compiling-X-13ARIMA-SEATS-from-Source-for-OS-X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels as sm\n",
    "import fredpy as fp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,urllib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# You must change XPATH if you are running this script from anywhere other than the directory containing x13as.\n",
    "XPATH = os.getcwd()\n",
    "\n",
    "# Load fredpy api key\n",
    "fp.api_key = fp.load_api_key('fred_api_key.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unemployment Rate\n",
    "\n",
    "We construct an unemployment series from April 1929 through the most recent date available by concatenating four U.S. unemployment rate series; all of which are available from FRED (https://fred.stlouisfed.org/). Specifically:\n",
    "\n",
    "1. Seasonally adjusted unemployment rate for the United States from April 1929 through February 1940. FRED series ID: M0892AUSM156SNBR. NBER Indicator: m08292a. \n",
    "2. Seasonally adjusted unemployment rate for the United States from March 1940 through December 1946. FRED series ID: M0892BUSM156SNBR. NBER Indicator: m08292b. \n",
    "3. Seasonally adjusted unemployment rate for the United States from January 1947 through  December 1947. FRED series ID: M0892CUSM156NNBR. NBER Indicator: m08292c.  Note: The source data are not seasonally adjusted and contain observations through December 1966. We seasonally adjust the entire series through December 1966 using the U.S. Census Bureau's X-12-ARIMA seasonal adjustment program. We then discard values after December 1947.\n",
    "4. Seasonally adjusted unemployment rate for the United States from January 1948 through the most recent date available. FRED series ID: UNRATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical US unemployment rate from the NBER Macrohistory Database: 1929-04-01 to 1940-02-01;\n",
    "# Seasonallyadjusted\n",
    "\n",
    "# Download from FRED and save as a Pandas series\n",
    "unemp_1 = fp.series('M0892AUSM156SNBR')\n",
    "unemp_1 = unemp_1.window(['04-01-1929','02-01-1940']).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical US unemployment rate from the NBER Macrohistory Database: 1940-03-01 to 1946-12-01;\n",
    "# Seasonally  adjusted\n",
    "\n",
    "# Download from FRED and save as a Pandas series\n",
    "unemp_2 = fp.series('M0892BUSM156SNBR')\n",
    "unemp_2 = unemp_2.window(['03-01-1940','12-01-1946']).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "X13NotFoundError",
     "evalue": "x12a and x13as not found on path. Give the path, put them on PATH, or set the X12PATH or X13PATH environmental variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mX13NotFoundError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0667dc999626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Run x13_arima_analysis to obtain SA unemployment data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mx13results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx13\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx13_arima_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munemp_3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx12path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutlier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_stdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0munemp_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx13results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseasadj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munemp_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tsa/x13.py\u001b[0m in \u001b[0;36mx13_arima_analysis\u001b[0;34m(endog, maxorder, maxdiff, diff, exog, log, outlier, trading, forecast_years, retspec, speconly, start, freq, print_stdout, x12path, prefer_x13)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0mback\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[0;32m--> 414\u001b[0;31m     \u001b[0mx12path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_x12\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx12path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tsa/x13.py\u001b[0m in \u001b[0;36m_check_x12\u001b[0;34m(x12path)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mx12path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_x12\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx12path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mx12path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         raise X13NotFoundError(\"x12a and x13as not found on path. Give the \"\n\u001b[0m\u001b[1;32m     87\u001b[0m                                \u001b[0;34m\"path, put them on PATH, or set the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                                \"X12PATH or X13PATH environmental variable.\")\n",
      "\u001b[0;31mX13NotFoundError\u001b[0m: x12a and x13as not found on path. Give the path, put them on PATH, or set the X12PATH or X13PATH environmental variable."
     ]
    }
   ],
   "source": [
    "# Historical US unemployment rate from the NBER Macrohistory Database: 1947-01-01 to 1966-12-01;\n",
    "# Raw series is *not* seasonally adjusted\n",
    "\n",
    "# Download from FRED\n",
    "unemp_3 = fp.series('M0892CUSM156NNBR')\n",
    "unemp_3 = unemp_3.window(['01-01-1947','12-01-1966']).data\n",
    "\n",
    "# Run x13_arima_analysis to obtain SA unemployment data.\n",
    "x13results = sm.tsa.x13.x13_arima_analysis(endog = unemp_3,x12path=XPATH, outlier=False,print_stdout=True)\n",
    "\n",
    "unemp_3 = pd.Series(x13results.seasadj.values,index=unemp_3.index)\n",
    "unemp_3 = unemp_3[(unemp_3.index>=pd.to_datetime('01-01-1947')) & (unemp_3.index<=pd.to_datetime('12-01-1947'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US civilian unemployment rate from the BLS: 1948-01-01 to most recent;\n",
    "# Seasonally  adjusted\n",
    "unemp_4 = fp.series('UNRATE')\n",
    "unemp_4 = unemp_4.window(['01-01-1948','01-01-2200']).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the series\n",
    "unemployment_rate_series = unemp_1.append(unemp_2).sort_index()\n",
    "unemployment_rate_series = unemployment_rate_series.append(unemp_3).sort_index()\n",
    "unemployment_rate_series = unemployment_rate_series.append(unemp_4).sort_index()\n",
    "\n",
    "# plot the series and save the figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(unemployment_rate_series,'-',lw=4,alpha = 0.65)\n",
    "ax.set_ylabel('Percent')\n",
    "ax.grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('../png/fig_data_unrate.png',bbox_inches='tight',dpi=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Vacancies (Job openings)\n",
    "\n",
    "We construct a series of vacancies for the United States going back to April 1929 by scaling and concatenating three series:\n",
    "1. Help-wanted advertising in newspapers index for United States from April 1929 to January 1960. FRED series ID: M0882AUSM349NNBR. NBER Indicator: m08082a. Note: The source data are not seasonally adjusted and contain observations through August 1960. We seasonally adjust the entire series through August 1960 using the United States Census Bureau's X-12-ARIMA seasonal adjustment program. We then discard values after January 1960.\n",
    "2. Composite help-wanted index from January 1960 through January 2001 constructed using the method described in and Barnichon (2010). We obtained the data from Barnichon's website https://sites.google.com/site/regisbarnichon/data. We scale this series so that its value in January 1960 equals the value of the NBER's help-wanted index for the same date.\n",
    "3. Job openings, total nonfarm for the United States from January 2001 to the most recent date available. FRED series ID: JTSJOL. We scale this series so that its value in January 2001 equals the value of the scaled help-wanted index from Barnichon for the same date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Met life help-wanted index: 1919-01-01 to 1960-08-01;\n",
    "# Not seasonally adjusted\n",
    "\n",
    "vac_1 = fp.series('M0882AUSM349NNBR').data\n",
    "\n",
    "# temp_series = pd.Series(vac_1.data,index=pd.to_datetime(vac_1.dates))\n",
    "\n",
    "# Run x13_arima_analysis to obtain SA vacancy rate data.\n",
    "x13results = sm.tsa.x13.x13_arima_analysis(endog = vac_1,x12path=XPATH, outlier=False,print_stdout=True)\n",
    "\n",
    "vac_1 = pd.Series(x13results.seasadj.values,index=vac_1.index)\n",
    "vac_1 = vac_1[(vac_1.index>=pd.to_datetime('04-01-1929')) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite help-wanted index from Regis Barnichon's site: https://sites.google.com/site/regisbarnichon;\n",
    "# Seasonally adjusted\n",
    "\n",
    "# Import data from Regis Barnichon's site\n",
    "dls = 'https://sites.google.com/site/regisbarnichon/cv/HWI_index.txt?attredirects=0'\n",
    "try:\n",
    "    urllib.urlretrieve(dls, '../txt/HWI_index.txt')\n",
    "except:\n",
    "    try:\n",
    "        urllib.request.urlretrieve(dls, '../txt/HWI_index.txt')\n",
    "    except:\n",
    "        print('HWI_index.txt is no longer available at given URL')\n",
    "\n",
    "vac_2 = pd.read_csv('../txt/HWI_index.txt',delimiter='\\t',skiprows=6)\n",
    "vac_2.columns = ['Date','composite HWI']\n",
    "\n",
    "# Manage dates\n",
    "dates = []\n",
    "for d in vac_2['Date']:\n",
    "    dates.append(d[-2:]+'-01-'+d[0:4])\n",
    "\n",
    "vac_2 = pd.Series(vac_2['composite HWI'].values,index = pd.to_datetime(dates))\n",
    "\n",
    "# Compute a scaling factor to ensure that the January 1, 1960 values of the first vacancy series match \n",
    "# the second.\n",
    "scaling = vac_1.loc['01-01-1960']/vac_2.loc['1960-01-01']\n",
    "vac_2 = scaling* vac_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job Openings and Labor Turnover Survey (JOLTS) : December 1, 2000 to present\n",
    "# Seasonally adjusted\n",
    "\n",
    "vac_3 = fp.series('JTSJOL').data\n",
    "\n",
    "# Compute a scaling factor to ensure that the December 1, 2000 values of the first vacancy series match \n",
    "# the second.\n",
    "scaling = vac_2.loc['12-01-2000']/vac_3.loc['12-01-2000']\n",
    "vac_3 = scaling* vac_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate each series\n",
    "vac_1 = vac_1.loc[:'12-01-1959']\n",
    "vac_2 = vac_2.loc['01-01-1960':'12-01-2000']\n",
    "vac_3 = vac_3.loc['01-01-2001':]\n",
    "\n",
    "# Plot the three truncated and scaled series to verify that they line up\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(vac_1,'-',lw=3,alpha = 0.65)\n",
    "ax.plot(vac_2,'-',lw=3,alpha = 0.65)\n",
    "ax.plot(vac_3,'-',lw=3,alpha = 0.65)\n",
    "ax.set_title('Vacancies (unscaled)')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vacancy series\n",
    "vacancy_series_unscaled = vac_1.append(vac_2).sort_index()\n",
    "vacancy_series_unscaled = vacancy_series_unscaled.append(vac_3).sort_index()\n",
    "\n",
    "# plot the series and save the figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot_date(vacancy_series_unscaled.index,vacancy_series_unscaled.values,'-',lw=3,alpha = 0.65)\n",
    "ax.set_title('Vacancies (unscaled)')\n",
    "ax.grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('../png/fig_data_vacancies.png',bbox_inches='tight',dpi=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labor force data\n",
    "\n",
    "Next, we construct monthly labor force data for the United States from April 1929 by concatenating two series:\n",
    "1. Civilian labor force for the United States from January 1948 to the most recent date available. FRED series ID: CLF16OV.\n",
    "2. Historical national population estimates from  Population Estimates Program, Population Division, U.S. Census Bureau. The source data are annual from July 1, 1900 to July 1, 1999 and not seasonally adjusted. We extend the data to monthly frequency by linear interpolation and discard observations before April 1929 and after January 1948. Then we scale this series so that its value in January 1948 equals the value of the civilian labor force series for the same date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Civilian labor force over 16 years of age in thousands of persons: January 1948 to present;\n",
    "# Seasonally adjusted\n",
    "lf_1 = fp.series('CLF16OV')\n",
    "lf_1 = lf_1.window(['01-01-1800','06-01-2216']).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Historical National Population Estimates:  July 1, 1900 to July 1, 1999\n",
    "# Source: Population Estimates Program, Population Division, U.S. Census Bureau\n",
    "# Annual, Not seasonally adjusted\n",
    "\n",
    "# Retrieve data from Census\n",
    "dls = 'http://www.census.gov/popest/data/national/totals/pre-1980/tables/popclockest.txt'\n",
    "dls = 'https://www.census.gov/population/estimates/nation/popclockest.txt'\n",
    "\n",
    "try:\n",
    "    urllib.urlretrieve(dls, '../txt/popclockest.txt')\n",
    "except:\n",
    "    try:\n",
    "        urllib.request.urlretrieve(dls, '../txt/popclockest.txt')\n",
    "    except:\n",
    "        print('popclockest.txt is no longer available at given URL')\n",
    "\n",
    "# Import data and edit file\n",
    "with open('../txt/popclockest.txt','r') as newfile:\n",
    "    lines = newfile.readlines()\n",
    "    \n",
    "# Remove leading and trailing whitespace and overwrite spaces in with tabs in lines\n",
    "newlines = []\n",
    "for i,line in enumerate(lines):\n",
    "    \n",
    "    newline = line.rstrip().lstrip()\n",
    "    newline = newline.replace('              ','\\t')\n",
    "    newline = newline.replace('          ','\\t')\n",
    "    newline = newline.replace('      ','\\t')\n",
    "    newline = newline+'\\n'\n",
    "    newlines.append(newline)\n",
    "\n",
    "# Collect the population and date information\n",
    "pop = []\n",
    "dates=[]\n",
    "for i,line in enumerate(newlines[9:]):\n",
    "    \n",
    "    if len(line.split('\\t'))==4:\n",
    "        line_split = line.split('\\t')\n",
    "        dates.append(line_split[0])\n",
    "        pop.append(float(line_split[1].replace(',','')))\n",
    "\n",
    "# Form the series\n",
    "lf_2 = pd.Series(pop,index = pd.to_datetime(dates))\n",
    "\n",
    "# Resample data as monthly and interpolate\n",
    "lf_2 = lf_2.sort_index()\n",
    "lf_2 = lf_2.resample('M').mean().interpolate()\n",
    "\n",
    "# Set dates to begining of month instead of middle\n",
    "lf_2.index = lf_2.index + pd.offsets.MonthBegin(0)\n",
    "\n",
    "# Compute a scaling factor to ensure that the Jaunary 1, 1948 values of the first LF series match \n",
    "# the second.\n",
    "scaling = lf_1.iloc[0]/lf_2[lf_2.index==pd.to_datetime('1948-01-01')].values[0]\n",
    "lf_2 = scaling*lf_2[(lf_2.index>=pd.to_datetime('1929-04-01')) & (lf_2.index<pd.to_datetime('1948-01-01'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two truncated and scaled series to verify that they line up\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(lf_2,'-',lw=3,alpha = 0.65)\n",
    "ax.plot(lf_1,'-',lw=3,alpha = 0.65)\n",
    "ax.set_title('Labor force')\n",
    "ax.grid()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# form the labor force series\n",
    "labor_force_series = lf_1.append(lf_2).sort_index()\n",
    "\n",
    "# plot the series and save the figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(labor_force_series/1000,'-',lw=4,alpha = 0.65)\n",
    "ax.set_title('Labor force')\n",
    "ax.set_ylabel('Millions of persons')\n",
    "ax.grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('../png/fig_data_labor_force.png',bbox_inches='tight',dpi=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vacancy rate\n",
    "\n",
    "Now that we have a vacancy series and a labor force series, we compute the monthly vacancy rate for the Unite States by dividing the vacancy rate series by the labor force series. Following Petrosky-Nadeau and Zhang (2013), we scale the result so that the average vacancy rate for 1965 is 2.05\\% in order to match the vacancy rate estimate for 1965 obtained by Zagorsky (1998)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the vacancy_rate series\n",
    "vacancy_rate_series = vacancy_series_unscaled / labor_force_series\n",
    "\n",
    "# Compute a scaling factor to ensure that the average vacancy rate for 1965 is 0.0205\n",
    "scaling = vacancy_rate_series[(vacancy_rate_series.index>=pd.to_datetime('1965-01-01')) & (vacancy_rate_series.index<=pd.to_datetime('1965-12-01'))].mean()/0.0205\n",
    "\n",
    "vacancy_rate_series = 100*vacancy_rate_series/scaling\n",
    "\n",
    "vacancy_series = vacancy_rate_series*labor_force_series/100\n",
    "unemployment_series = unemployment_rate_series*labor_force_series/100\n",
    "market_tightness_series = vacancy_series/unemployment_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the series and save the figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(vacancy_rate_series,'-',lw=4,alpha = 0.65)\n",
    "ax.set_ylabel('Vacancy rate')\n",
    "ax.grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('../png/fig_data_vacancy_rate.png',bbox_inches='tight',dpi=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize data\n",
    "\n",
    "In the rest of the program, we organize the data into dataframes, construct plots that we use in our paper, and export datasets that can be used to replicate our figures and to investigate carefully the data more carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data into DataFrames\n",
    "df_rates = pd.concat([unemployment_rate_series,vacancy_rate_series,market_tightness_series], join='outer', axis = 1).dropna()\n",
    "df_rates.columns = ['Unemployment rate','Vacancy rate','Market tightness']\n",
    "\n",
    "df_levels= pd.concat([unemployment_series,labor_force_series,vacancy_series], join='outer', axis = 1).dropna()\n",
    "df_levels.columns = ['Unemployment [Thousands of persons]','Labor force [Thousands of persons]','Vacancies [Thousands of vacancies]']\n",
    "\n",
    "df_all = pd.concat([df_rates,df_levels], join='outer', axis = 1).dropna()\n",
    "\n",
    "# Subframes for pre December 2007 and after\n",
    "df_pre_gr = df_all[(df_all.index< '12-01-2007')]\n",
    "df_post_gr = df_all[(df_all.index>= '12-01-2007')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the labor market tightness series and save the figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot_date(df_all.index,df_all['Market tightness'].values,'-',lw=4,alpha = 0.65)\n",
    "# plt.scatter(df_all['Unemployment rate'].values,df_all['Market tightness'].values,s=45,c= 'blue',alpha = 0.25)\n",
    "ax.set_ylabel('Labor market tightness')\n",
    "ax.set_ylim([0,5])\n",
    "ax.grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('../png/fig_data_market_tightness.png',bbox_inches='tight',dpi=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Beveridge curve for the US: vacancy rate v unemployment rate\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "c = np.arange(len(df_all.index))\n",
    "plt.scatter(df_all['Unemployment rate'].values,df_all['Vacancy rate'].values,s=45,c= c,alpha = 0.35)\n",
    "ax.set_xlim([0,30])\n",
    "ax.set_ylim([0,6])\n",
    "# ax.set_title('Beveridge curve')\n",
    "ax.set_xlabel('Unemployment rate')\n",
    "ax.set_ylabel('Vacancy rate')\n",
    "ax.grid()\n",
    "\n",
    "plt.savefig('../png/fig_beveridge_curve.png',bbox_inches='tight',dpi=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the modified Beveridge curve for the US: market tightness v unemployment rate\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "c = np.arange(len(df_all.index))\n",
    "plt.scatter(df_all['Unemployment rate'].values,df_all['Market tightness'].values,s=45,c= 'blue',alpha = 0.25)\n",
    "ax.set_xlim([-0.5,26])\n",
    "ax.set_ylim([-0.5,5])\n",
    "# ax.set_title('Modified Beveridge curve')\n",
    "ax.set_xlabel('Unemployment rate ($\\%$)')\n",
    "ax.set_ylabel('Market tightness ($\\\\theta$)')\n",
    "ax.grid()\n",
    "\n",
    "plt.savefig('../png/fig_modified_beveridge_curve.png',bbox_inches='tight',dpi=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Construct figure for paper\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "c = np.arange(len(df_all.index))\n",
    "plt.scatter(df_all['Unemployment rate'].values,df_all['Market tightness'].values,s=45,c= 'blue',alpha = 0.25)\n",
    "ax.set_xlim([-0.5,26])\n",
    "ax.set_ylim([-0.5,5])\n",
    "ax.set_title(df_all.index[0].strftime('%B %Y')+' to '+df_all.index[-1].strftime('%B %Y'))\n",
    "ax.set_xlabel('Unemployment rate ($\\%$)')\n",
    "ax.set_ylabel('Market tightness ($\\\\theta$)')\n",
    "ax.grid()\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "\n",
    "c = np.arange(len(df_post_gr.index))\n",
    "\n",
    "plt.scatter(df_post_gr['Unemployment rate'].values,df_post_gr['Market tightness'].values,s=75,alpha = 0.5,c=c)\n",
    "\n",
    "cbar = plt.colorbar(ax = ax)\n",
    "cbar.set_ticks([int(i) for i in cbar.get_ticks()])\n",
    "cbar.set_ticklabels([df_post_gr.index[int(i)].strftime('%b %Y') for i in cbar.get_ticks()[:-1]])\n",
    "\n",
    "plt.plot(df_post_gr['Unemployment rate'].values,df_post_gr['Market tightness'].values,'-')\n",
    "\n",
    "ax.set_title(df_post_gr.index[0].strftime('%b %Y')+' to '+df_post_gr.index[-1].strftime('%B %Y'))\n",
    "ax.set_xlabel('Unemployment rate ($u$)')\n",
    "ax.set_ylabel('Market tightness ($\\\\theta$)')\n",
    "ax.grid()\n",
    "\n",
    "plt.savefig('../png/fig_modified_beveridge_curve_both.png',bbox_inches='tight',dpi=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Export data to csv\n",
    "df_levels.to_csv('../csv/beveridge_curve_data.csv',index_label='Date',float_format='%11.2f')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
